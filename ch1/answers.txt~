Answers to exercises in chapter 1

1.1

1.2

1.3 Here's a somewhat hacky proof. The squared error E is defined as follows:
We can think of the <b, v_TRAIN(b)> as points in the continuous function
E = sum of (v_TRAIN(b) - v(b))^2, and so the only variable component of this 
function is the v(b). We have the linear function v: R^n -> R, so its
derivative wrt any x_i is just the constant w_i. Therefore del v/del w_i = x_i.
Thus if we take del E/del w_i, we get (- v(b))*(del v/del w_i) = sum over b's 
(-v(b))*x_i. Now the gradient descent rule updates each weight w_i 
proportionally to x_i, so the derivative is ax_i for each weight w_i, and some 
scalar a. Thus the derivative del E/ del w_i is proportional to the LMS update
rule, so this is an example of gradient descent. 


1.4
 
1.5 See code in the local directory tictactoe. Main module is learner.py
